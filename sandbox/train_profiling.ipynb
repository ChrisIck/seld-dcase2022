{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A wrapper script that trains the SELDnet. The training stops when the early stopping metric - SELD error stops improving.\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import cls_feature_class\n",
    "import cls_data_generator\n",
    "import seldnet_model\n",
    "import parameters\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "plot.switch_backend('agg')\n",
    "from IPython import embed\n",
    "from cls_compute_seld_results import ComputeSELDResults, reshape_3Dto2D\n",
    "from SELD_evaluation_metrics import distance_between_cartesian_coordinates\n",
    "import seldnet_model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accdoa_labels(accdoa_in, nb_classes):\n",
    "    x, y, z = accdoa_in[:, :, :nb_classes], accdoa_in[:, :, nb_classes:2*nb_classes], accdoa_in[:, :, 2*nb_classes:]\n",
    "    sed = np.sqrt(x**2 + y**2 + z**2) > 0.5\n",
    "      \n",
    "    return sed, accdoa_in\n",
    "\n",
    "\n",
    "def get_multi_accdoa_labels(accdoa_in, nb_classes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        accdoa_in:  [batch_size, frames, num_track*num_axis*num_class=3*3*12]\n",
    "        nb_classes: scalar\n",
    "    Return:\n",
    "        sedX:       [batch_size, frames, num_class=12]\n",
    "        doaX:       [batch_size, frames, num_axis*num_class=3*12]\n",
    "    \"\"\"\n",
    "    x0, y0, z0 = accdoa_in[:, :, :1*nb_classes], accdoa_in[:, :, 1*nb_classes:2*nb_classes], accdoa_in[:, :, 2*nb_classes:3*nb_classes]\n",
    "    sed0 = np.sqrt(x0**2 + y0**2 + z0**2) > 0.5\n",
    "    doa0 = accdoa_in[:, :, :3*nb_classes]\n",
    "\n",
    "    x1, y1, z1 = accdoa_in[:, :, 3*nb_classes:4*nb_classes], accdoa_in[:, :, 4*nb_classes:5*nb_classes], accdoa_in[:, :, 5*nb_classes:6*nb_classes]\n",
    "    sed1 = np.sqrt(x1**2 + y1**2 + z1**2) > 0.5\n",
    "    doa1 = accdoa_in[:, :, 3*nb_classes: 6*nb_classes]\n",
    "\n",
    "    x2, y2, z2 = accdoa_in[:, :, 6*nb_classes:7*nb_classes], accdoa_in[:, :, 7*nb_classes:8*nb_classes], accdoa_in[:, :, 8*nb_classes:]\n",
    "    sed2 = np.sqrt(x2**2 + y2**2 + z2**2) > 0.5\n",
    "    doa2 = accdoa_in[:, :, 6*nb_classes:]\n",
    "\n",
    "    return sed0, doa0, sed1, doa1, sed2, doa2\n",
    "\n",
    "\n",
    "def determine_similar_location(sed_pred0, sed_pred1, doa_pred0, doa_pred1, class_cnt, thresh_unify, nb_classes):\n",
    "    if (sed_pred0 == 1) and (sed_pred1 == 1):\n",
    "        if distance_between_cartesian_coordinates(doa_pred0[class_cnt], doa_pred0[class_cnt+1*nb_classes], doa_pred0[class_cnt+2*nb_classes],\n",
    "                                                  doa_pred1[class_cnt], doa_pred1[class_cnt+1*nb_classes], doa_pred1[class_cnt+2*nb_classes]) < thresh_unify:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def test_epoch(data_generator, model, criterion, dcase_output_folder, params, device):\n",
    "    # Number of frames for a 60 second audio with 100ms hop length = 600 frames\n",
    "    # Number of frames in one batch (batch_size* sequence_length) consists of all the 600 frames above with zero padding in the remaining frames\n",
    "    test_filelist = data_generator.get_filelist()\n",
    "\n",
    "    nb_test_batches, test_loss = 0, 0.\n",
    "    model.eval()\n",
    "    file_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_generator.generate():\n",
    "            # load one batch of data\n",
    "            data, target = torch.tensor(data).to(device).float(), torch.tensor(target).to(device).float()\n",
    "\n",
    "            # process the batch of data based on chosen mode\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            if params['multi_accdoa'] is True:\n",
    "                sed_pred0, doa_pred0, sed_pred1, doa_pred1, sed_pred2, doa_pred2 = get_multi_accdoa_labels(output.detach().cpu().numpy(), params['unique_classes'])\n",
    "                sed_pred0 = reshape_3Dto2D(sed_pred0)\n",
    "                doa_pred0 = reshape_3Dto2D(doa_pred0)\n",
    "                sed_pred1 = reshape_3Dto2D(sed_pred1)\n",
    "                doa_pred1 = reshape_3Dto2D(doa_pred1)\n",
    "                sed_pred2 = reshape_3Dto2D(sed_pred2)\n",
    "                doa_pred2 = reshape_3Dto2D(doa_pred2)\n",
    "            else:\n",
    "                sed_pred, doa_pred = get_accdoa_labels(output.detach().cpu().numpy(), params['unique_classes'])\n",
    "                sed_pred = reshape_3Dto2D(sed_pred)\n",
    "                doa_pred = reshape_3Dto2D(doa_pred)\n",
    "\n",
    "            # dump SELD results to the correspondin file\n",
    "            output_file = os.path.join(dcase_output_folder, test_filelist[file_cnt].replace('.npy', '.csv'))\n",
    "            file_cnt += 1\n",
    "            output_dict = {}\n",
    "            if params['multi_accdoa'] is True:\n",
    "                for frame_cnt in range(sed_pred0.shape[0]):\n",
    "                    for class_cnt in range(sed_pred0.shape[1]):\n",
    "                        # determine whether track0 is similar to track1\n",
    "                        flag_0sim1 = determine_similar_location(sed_pred0[frame_cnt][class_cnt], sed_pred1[frame_cnt][class_cnt], doa_pred0[frame_cnt], doa_pred1[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        flag_1sim2 = determine_similar_location(sed_pred1[frame_cnt][class_cnt], sed_pred2[frame_cnt][class_cnt], doa_pred1[frame_cnt], doa_pred2[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        flag_2sim0 = determine_similar_location(sed_pred2[frame_cnt][class_cnt], sed_pred0[frame_cnt][class_cnt], doa_pred2[frame_cnt], doa_pred0[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        # unify or not unify according to flag\n",
    "                        if flag_0sim1 + flag_1sim2 + flag_2sim0 == 0:\n",
    "                            if sed_pred0[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred0[frame_cnt][class_cnt], doa_pred0[frame_cnt][class_cnt+params['unique_classes']], doa_pred0[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                            if sed_pred1[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred1[frame_cnt][class_cnt], doa_pred1[frame_cnt][class_cnt+params['unique_classes']], doa_pred1[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                            if sed_pred2[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred2[frame_cnt][class_cnt], doa_pred2[frame_cnt][class_cnt+params['unique_classes']], doa_pred2[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                        elif flag_0sim1 + flag_1sim2 + flag_2sim0 == 1:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            if flag_0sim1:\n",
    "                                if sed_pred2[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred2[frame_cnt][class_cnt], doa_pred2[frame_cnt][class_cnt+params['unique_classes']], doa_pred2[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred0[frame_cnt] + doa_pred1[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                            elif flag_1sim2:\n",
    "                                if sed_pred0[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred0[frame_cnt][class_cnt], doa_pred0[frame_cnt][class_cnt+params['unique_classes']], doa_pred0[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred1[frame_cnt] + doa_pred2[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                            elif flag_2sim0:\n",
    "                                if sed_pred1[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred1[frame_cnt][class_cnt], doa_pred1[frame_cnt][class_cnt+params['unique_classes']], doa_pred1[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred2[frame_cnt] + doa_pred0[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                        elif flag_0sim1 + flag_1sim2 + flag_2sim0 >= 2:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            doa_pred_fc = (doa_pred0[frame_cnt] + doa_pred1[frame_cnt] + doa_pred2[frame_cnt]) / 3\n",
    "                            output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "            else:\n",
    "                for frame_cnt in range(sed_pred.shape[0]):\n",
    "                    for class_cnt in range(sed_pred.shape[1]):\n",
    "                        if sed_pred[frame_cnt][class_cnt]>0.5:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            output_dict[frame_cnt].append([class_cnt, doa_pred[frame_cnt][class_cnt], doa_pred[frame_cnt][class_cnt+params['unique_classes']], doa_pred[frame_cnt][class_cnt+2*params['unique_classes']]]) \n",
    "            data_generator.write_output_format_file(output_file, output_dict)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            nb_test_batches += 1\n",
    "            if params['quick_test'] and nb_test_batches == 4:\n",
    "                break\n",
    "\n",
    "\n",
    "        test_loss /= nb_test_batches\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_epoch(data_generator, optimizer, model, criterion, params, device):\n",
    "    nb_train_batches, train_loss = 0, 0.\n",
    "    model.train()\n",
    "    for data, target in data_generator.generate():\n",
    "        # load one batch of data\n",
    "        data, target = torch.tensor(data).to(device).float(), torch.tensor(target).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # process the batch of data based on chosen mode\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        nb_train_batches += 1\n",
    "        if params['quick_test'] and nb_train_batches == 4:\n",
    "            break\n",
    "\n",
    "    train_loss /= nb_train_batches\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '1', 'train_profile']\n",
      "Using CPU\n",
      "SET: 1\n",
      "USING DEFAULT PARAMETERS\n",
      "\n",
      "Loading data from /scratch/ci411/SELD_Datasets/TNR_0518\n",
      "\tquick_test: False\n",
      "\tfinetune_mode: False\n",
      "\tpretrained_model_weights: None\n",
      "\tdataset_dir: /scratch/ci411/SELD_Datasets/TNR_0518\n",
      "\tfeat_label_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518\n",
      "\tmodel_dir: models/\n",
      "\tdcase_output_dir: results/\n",
      "\tmode: dev\n",
      "\tdataset: mic\n",
      "\tfs: 24000\n",
      "\thop_len_s: 0.02\n",
      "\tlabel_hop_len_s: 0.1\n",
      "\tmax_audio_len_s: 60\n",
      "\tnb_mel_bins: 64\n",
      "\tuse_salsalite: False\n",
      "\tfmin_doa_salsalite: 50\n",
      "\tfmax_doa_salsalite: 2000\n",
      "\tfmax_spectra_salsalite: 9000\n",
      "\tmulti_accdoa: False\n",
      "\tthresh_unify: 15\n",
      "\tlabel_sequence_length: 50\n",
      "\tbatch_size: 256\n",
      "\tdropout_rate: 0.05\n",
      "\tnb_cnn2d_filt: 64\n",
      "\tf_pool_size: [4, 4, 2]\n",
      "\tnb_rnn_layers: 2\n",
      "\trnn_size: 128\n",
      "\tself_attn: False\n",
      "\tnb_heads: 4\n",
      "\tnb_fnn_layers: 1\n",
      "\tfnn_size: 128\n",
      "\tnb_epochs: 100\n",
      "\tlr: 0.001\n",
      "\taverage: macro\n",
      "\tlad_doa_thresh: 20\n",
      "\tfeature_sequence_length: 250\n",
      "\tt_pool_size: [5, 1, 1]\n",
      "\tpatience: 100\n",
      "\tunique_classes: 14\n",
      "Using Train 1, Val 2, Test 3,4\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "------------------------------------      SPLIT [3, 4]   -----------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "unique_name: 1_train_profile_dev_split0_accdoa_mic_gcc\n",
      "\n",
      "Loading training dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tDatagen_mode: dev, nb_files: 900, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [1]\n",
      "\tbatch_size: 256, feat_seq_len: 250, label_seq_len: 50, shuffle: True\n",
      "\tTotal batches in dataset: 42\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "Loading validation dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tWARNING: Resetting batch size to 12. To accommodate the inference of longest file of 3000 frames in a single batch\n",
      "\tDatagen_mode: dev, nb_files: 300, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [2]\n",
      "\tbatch_size: 12, feat_seq_len: 250, label_seq_len: 50, shuffle: False\n",
      "\tTotal batches in dataset: 300\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "---------------- SELD-net -------------------\n",
      "FEATURES:\n",
      "\tdata_in: (256, 10, 250, 64)\n",
      "\tdata_out: (256, 50, 42)\n",
      "\n",
      "MODEL:\n",
      "\tdropout_rate: 0.05\n",
      "\tCNN: nb_cnn_filt: 64, f_pool_size[4, 4, 2], t_pool_size[5, 1, 1]\n",
      "\trnn_size: 128, fnn_size: 128\n",
      "\n",
      "CRNN(\n",
      "  (conv_block_list): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(10, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=(5, 4), stride=(5, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Dropout2d(p=0.05, inplace=False)\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout2d(p=0.05, inplace=False)\n",
      "    (6): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout2d(p=0.05, inplace=False)\n",
      "  )\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.05, bidirectional=True)\n",
      "  (fnn_list): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=42, bias=True)\n",
      "  )\n",
      ")\n",
      "Dumping recording-wise val results in: results/1_train_profile_dev_split0_accdoa_mic_gcc_20230524153927_val\n"
     ]
    }
   ],
   "source": [
    "argv = ['','1','train_profile']\n",
    "print(argv)\n",
    "if len(argv) != 3:\n",
    "    print('\\n\\n')\n",
    "    print('-------------------------------------------------------------------------------------------------------')\n",
    "    print('The code expected two optional inputs')\n",
    "    print('\\t>> python seld.py <task-id> <job-id>')\n",
    "    print('\\t\\t<task-id> is used to choose the user-defined parameter set from parameter.py')\n",
    "    print('Using default inputs for now')\n",
    "    print('\\t\\t<job-id> is a unique identifier which is used for output filenames (models, training plots). '\n",
    "          'You can use any number or string for this.')\n",
    "    print('-------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n\\n')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# use parameter set defined by user\n",
    "task_id = '1' if len(argv) < 2 else argv[1]\n",
    "params = parameters.get_params(task_id)\n",
    "params['quick_test'] = True\n",
    "\n",
    "job_id = 1 if len(argv) < 3 else argv[-1]\n",
    "\n",
    "# Training setup\n",
    "train_splits, val_splits, test_splits = None, None, None\n",
    "if params['mode'] == 'dev':\n",
    "    if '2020' in params['dataset_dir']:\n",
    "        test_splits = [1]\n",
    "        val_splits = [2]\n",
    "        train_splits = [[3, 4, 5, 6]]\n",
    "\n",
    "    elif '2021' in params['dataset_dir']:\n",
    "        test_splits = [6]\n",
    "        val_splits = [5]\n",
    "        train_splits = [[1, 2, 3, 4]]\n",
    "\n",
    "    elif '2022' in params['dataset_dir']:\n",
    "        test_splits = [[4]]\n",
    "        val_splits = [[4]]\n",
    "        train_splits = [[1, 2, 3]] \n",
    "\n",
    "    else:\n",
    "        print('Using Train 1, Val 2, Test 3,4')\n",
    "        test_splits = [[3,4]]\n",
    "        val_splits = [[2]]\n",
    "        train_splits = [[1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for split_cnt, split in enumerate(test_splits):\n",
    "        \n",
    "        # Unique name for the run\n",
    "        loc_feat = params['dataset']\n",
    "        if params['dataset'] == 'mic':\n",
    "            if params['use_salsalite']:\n",
    "                loc_feat = '{}_salsa'.format(params['dataset'])\n",
    "            else:\n",
    "                loc_feat = '{}_gcc'.format(params['dataset'])\n",
    "        loc_output = 'multiaccdoa' if params['multi_accdoa'] else 'accdoa'\n",
    "\n",
    "        cls_feature_class.create_folder(params['model_dir'])\n",
    "        unique_name = '{}_{}_{}_split{}_{}_{}'.format(\n",
    "            task_id, job_id, params['mode'], split_cnt, loc_output, loc_feat\n",
    "        )\n",
    "        model_name = '{}_model.h5'.format(os.path.join(params['model_dir'], unique_name))\n",
    "        print(\"unique_name: {}\\n\".format(unique_name))\n",
    "\n",
    "        # Load train and validation data\n",
    "        print('Loading training dataset:')\n",
    "        data_gen_train = cls_data_generator.DataGenerator(\n",
    "            params=params, split=train_splits[split_cnt]\n",
    "        )\n",
    "\n",
    "        print('Loading validation dataset:')\n",
    "        data_gen_val = cls_data_generator.DataGenerator(\n",
    "            params=params, split=val_splits[split_cnt], shuffle=False, per_file=True\n",
    "        )\n",
    "\n",
    "        # Collect i/o data size and load model configuration\n",
    "        data_in, data_out = data_gen_train.get_data_sizes()\n",
    "        model = seldnet_model.CRNN(data_in, data_out, params).to(device)\n",
    "        if params['finetune_mode']:\n",
    "            print('Running in finetuning mode. Initializing the model to the weights - {}'.format(params['pretrained_model_weights']))\n",
    "            model.load_state_dict(torch.load(params['pretrained_model_weights'], map_location='cpu'))\n",
    "\n",
    "\n",
    "        # Dump results in DCASE output format for calculating final scores\n",
    "        dcase_output_val_folder = os.path.join(params['dcase_output_dir'], '{}_{}_val'.format(unique_name, strftime(\"%Y%m%d%H%M%S\", gmtime())))\n",
    "        cls_feature_class.delete_and_create_folder(dcase_output_val_folder)\n",
    "        print('Dumping recording-wise val results in: {}'.format(dcase_output_val_folder))\n",
    "\n",
    "        # Initialize evaluation metric class\n",
    "        score_obj = ComputeSELDResults(params)\n",
    "\n",
    "        # start training\n",
    "        best_val_epoch = -1\n",
    "        best_ER, best_F, best_LE, best_LR, best_seld_scr = 1., 0., 180., 0., 9999 \n",
    "        patience_cnt = 0\n",
    "\n",
    "        nb_epoch = 2 if params['quick_test'] else params['nb_epochs']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        if params['multi_accdoa'] is True:\n",
    "            criterion = seldnet_model.MSELoss_ADPIT()\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "        for epoch_cnt in range(nb_epoch):\n",
    "            # ---------------------------------------------------------------------\n",
    "            # TRAINING\n",
    "            # ---------------------------------------------------------------------\n",
    "            start_time = time.time()\n",
    "            train_loss = train_epoch(data_gen_train, optimizer, model, criterion, params, device)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # ---------------------------------------------------------------------\n",
    "            # VALIDATION\n",
    "            # ---------------------------------------------------------------------\n",
    "            start_time = time.time()\n",
    "            val_loss = test_epoch(data_gen_val, model, criterion, dcase_output_val_folder, params, device)\n",
    "\n",
    "            # Calculate the DCASE 2021 metrics - Location-aware detection and Class-aware localization scores\n",
    "            val_ER, val_F, val_LE, val_LR, val_seld_scr, classwise_val_scr = score_obj.get_SELD_Results(dcase_output_val_folder)\n",
    "\n",
    "            val_time = time.time() - start_time\n",
    "\n",
    "            # Save model if loss is good\n",
    "            if val_seld_scr <= best_seld_scr:\n",
    "                best_val_epoch, best_ER, best_F, best_LE, best_LR, best_seld_scr = epoch_cnt, val_ER, val_F, val_LE, val_LR, val_seld_scr\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "\n",
    "            # Print stats\n",
    "            print(\n",
    "                'epoch: {}, time: {:0.2f}/{:0.2f}, '\n",
    "                # 'train_loss: {:0.2f}, val_loss: {:0.2f}, '\n",
    "                'train_loss: {:0.4f}, val_loss: {:0.4f}, '\n",
    "                'ER/F/LE/LR/SELD: {}, '\n",
    "                'best_val_epoch: {} {}'.format(\n",
    "                    epoch_cnt, train_time, val_time,\n",
    "                    train_loss, val_loss,\n",
    "                    '{:0.2f}/{:0.2f}/{:0.2f}/{:0.2f}/{:0.2f}'.format(val_ER, val_F, val_LE, val_LR, val_seld_scr),\n",
    "                    best_val_epoch, '({:0.2f}/{:0.2f}/{:0.2f}/{:0.2f}/{:0.2f})'.format(best_ER, best_F, best_LE, best_LR, best_seld_scr))\n",
    "            )\n",
    "\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt > params['patience']:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "------------------------------------      SPLIT [3, 4]   -----------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "unique_name: 1_train_profile_dev_split0_accdoa_mic_gcc\n",
      "\n",
      "Loading training dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tDatagen_mode: dev, nb_files: 900, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [1]\n",
      "\tbatch_size: 256, feat_seq_len: 250, label_seq_len: 50, shuffle: True\n",
      "\tTotal batches in dataset: 42\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "Loading validation dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tWARNING: Resetting batch size to 12. To accommodate the inference of longest file of 3000 frames in a single batch\n",
      "\tDatagen_mode: dev, nb_files: 300, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [2]\n",
      "\tbatch_size: 12, feat_seq_len: 250, label_seq_len: 50, shuffle: False\n",
      "\tTotal batches in dataset: 300\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "---------------- SELD-net -------------------\n",
      "FEATURES:\n",
      "\tdata_in: (256, 10, 250, 64)\n",
      "\tdata_out: (256, 50, 42)\n",
      "\n",
      "MODEL:\n",
      "\tdropout_rate: 0.05\n",
      "\tCNN: nb_cnn_filt: 64, f_pool_size[4, 4, 2], t_pool_size[5, 1, 1]\n",
      "\trnn_size: 128, fnn_size: 128\n",
      "\n",
      "CRNN(\n",
      "  (conv_block_list): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(10, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=(5, 4), stride=(5, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Dropout2d(p=0.05, inplace=False)\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout2d(p=0.05, inplace=False)\n",
      "    (6): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout2d(p=0.05, inplace=False)\n",
      "  )\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.05, bidirectional=True)\n",
      "  (fnn_list): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=42, bias=True)\n",
      "  )\n",
      ")\n",
      "Dumping recording-wise val results in: results/1_train_profile_dev_split0_accdoa_mic_gcc_20230524155600_val\n",
      "epoch: 0, time: 27.05/0.27, train_loss: 0.0343, val_loss: 0.0331, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 0 (1.00/0.00/180.00/0.00/1.00)\n",
      "epoch: 1, time: 27.98/0.27, train_loss: 0.0304, val_loss: 0.0324, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.00/0.00/180.00/0.00/1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1 s\n",
       "\n",
       "Total time: 54.9878 s\n",
       "File: /state/partition1/job-33793231/ipykernel_2700888/2980855105.py\n",
       "Function: train_epoch at line 140\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   140                                           def train_epoch(data_generator, optimizer, model, criterion, params, device):\n",
       "   141         2          0.0      0.0      0.0      nb_train_batches, train_loss = 0, 0.\n",
       "   142         2          0.0      0.0      0.0      model.train()\n",
       "   143         8         12.2      1.5     22.2      for data, target in data_generator.generate():\n",
       "   144                                                   # load one batch of data\n",
       "   145         8          0.7      0.1      1.3          data, target = torch.tensor(data).to(device).float(), torch.tensor(target).to(device).float()\n",
       "   146         8          0.0      0.0      0.0          optimizer.zero_grad()\n",
       "   147                                           \n",
       "   148                                                   # process the batch of data based on chosen mode\n",
       "   149         8         21.1      2.6     38.3          output = model(data)\n",
       "   150                                                   \n",
       "   151         8          0.0      0.0      0.1          loss = criterion(output, target)\n",
       "   152         8         20.9      2.6     38.0          loss.backward()\n",
       "   153         8          0.0      0.0      0.0          optimizer.step()\n",
       "   154                                                   \n",
       "   155         8          0.0      0.0      0.0          train_loss += loss.item()\n",
       "   156         8          0.0      0.0      0.0          nb_train_batches += 1\n",
       "   157         6          0.0      0.0      0.0          if params['quick_test'] and nb_train_batches == 4:\n",
       "   158         2          0.0      0.0      0.1              break\n",
       "   159                                           \n",
       "   160         2          0.0      0.0      0.0      train_loss /= nb_train_batches\n",
       "   161                                           \n",
       "   162         2          0.0      0.0      0.0      return train_loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f train_epoch -u 1 train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tDatagen_mode: dev, nb_files: 900, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [1]\n",
      "\tbatch_size: 256, feat_seq_len: 250, label_seq_len: 50, shuffle: True\n",
      "\tTotal batches in dataset: 42\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_gen_train = cls_data_generator.DataGenerator(\n",
    "            params=params, split=train_splits[split_cnt]\n",
    "        )\n",
    "\n",
    "def gen_test(n):\n",
    "    run = 0\n",
    "    for _ in data_gen_train.generate():\n",
    "        run += 1\n",
    "        if run >= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1 s\n",
       "\n",
       "Total time: 56.9531 s\n",
       "File: /home/ci411/seld-dcase2022/sandbox/../cls_data_generator.py\n",
       "Function: generate at line 117\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   117                                               def generate(self):\n",
       "   118                                                   \"\"\"\n",
       "   119                                                   Generates batches of samples\n",
       "   120                                                   :return: \n",
       "   121                                                   \"\"\"\n",
       "   122         1          0.0      0.0      0.0          if self._shuffle:\n",
       "   123         1          0.0      0.0      0.0              random.shuffle(self._filenames_list)\n",
       "   124                                           \n",
       "   125                                                   # Ideally this should have been outside the while loop. But while generating the test data we want the data\n",
       "   126                                                   # to be the same exactly for all epoch's hence we keep it here.\n",
       "   127         1          0.0      0.0      0.0          self._circ_buf_feat = deque()\n",
       "   128         1          0.0      0.0      0.0          self._circ_buf_label = deque()\n",
       "   129                                           \n",
       "   130         1          0.0      0.0      0.0          file_cnt = 0\n",
       "   131         1          0.0      0.0      0.0          if self._is_eval:\n",
       "   132                                                       for i in range(self._nb_total_batches):\n",
       "   133                                                           # load feat and label to circular buffer. Always maintain atleast one batch worth feat and label in the\n",
       "   134                                                           # circular buffer. If not keep refilling it.\n",
       "   135                                                           while len(self._circ_buf_feat) < self._feature_batch_seq_len:\n",
       "   136                                                               temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
       "   137                                           \n",
       "   138                                                               for row_cnt, row in enumerate(temp_feat):\n",
       "   139                                                                   self._circ_buf_feat.append(row)\n",
       "   140                                           \n",
       "   141                                                               # If self._per_file is True, this returns the sequences belonging to a single audio recording\n",
       "   142                                                               if self._per_file:\n",
       "   143                                                                   extra_frames = self._feature_batch_seq_len - temp_feat.shape[0]\n",
       "   144                                                                   extra_feat = np.ones((extra_frames, temp_feat.shape[1])) * 1e-6\n",
       "   145                                           \n",
       "   146                                                                   for row_cnt, row in enumerate(extra_feat):\n",
       "   147                                                                       self._circ_buf_feat.append(row)\n",
       "   148                                           \n",
       "   149                                                               file_cnt = file_cnt + 1\n",
       "   150                                           \n",
       "   151                                                           # Read one batch size from the circular buffer\n",
       "   152                                                           feat = np.zeros((self._feature_batch_seq_len, self._nb_mel_bins * self._nb_ch))\n",
       "   153                                                           for j in range(self._feature_batch_seq_len):\n",
       "   154                                                               feat[j, :] = self._circ_buf_feat.popleft()\n",
       "   155                                                           feat = np.reshape(feat, (self._feature_batch_seq_len, self._nb_ch, self._nb_mel_bins))\n",
       "   156                                           \n",
       "   157                                                           # Split to sequences\n",
       "   158                                                           feat = self._split_in_seqs(feat, self._feature_seq_len)\n",
       "   159                                                           feat = np.transpose(feat, (0, 2, 1, 3))\n",
       "   160                                           \n",
       "   161                                                           yield feat\n",
       "   162                                           \n",
       "   163                                                   else:\n",
       "   164        42          0.0      0.0      0.0              for i in range(self._nb_total_batches):\n",
       "   165                                           \n",
       "   166                                                           # load feat and label to circular buffer. Always maintain atleast one batch worth feat and label in the\n",
       "   167                                                           # circular buffer. If not keep refilling it.\n",
       "   168       896          0.0      0.0      0.0                  while len(self._circ_buf_feat) < self._feature_batch_seq_len:\n",
       "   169       896         36.0      0.0     63.3                      temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
       "   170       896          8.3      0.0     14.6                      temp_label = np.load(os.path.join(self._label_dir, self._filenames_list[file_cnt]))\n",
       "   171       896          0.0      0.0      0.0                      if not self._per_file: \n",
       "   172                                                                   # Inorder to support variable length features, and labels of different resolution. \n",
       "   173                                                                   # We remove all frames in features and labels matrix that are outside \n",
       "   174                                                                   # the multiple of self._label_seq_len and self._feature_seq_len. Further we do this only in training.\n",
       "   175       896          0.0      0.0      0.0                          temp_label = temp_label[:temp_label.shape[0] - (temp_label.shape[0] % self._label_seq_len)]\n",
       "   176       896          0.0      0.0      0.0                          temp_mul = temp_label.shape[0]//self._label_seq_len\n",
       "   177       896          0.0      0.0      0.0                          temp_feat = temp_feat[:temp_mul*self._feature_seq_len, :]\n",
       "   178                                           \n",
       "   179   2688000          0.8      0.0      1.4                      for f_row in temp_feat:\n",
       "   180   2688000          0.7      0.0      1.2                          self._circ_buf_feat.append(f_row)\n",
       "   181    537600          0.2      0.0      0.3                      for l_row in temp_label:\n",
       "   182    537600          0.1      0.0      0.3                          self._circ_buf_label.append(l_row)\n",
       "   183                                                               \n",
       "   184                                                               # If self._per_file is True, this returns the sequences belonging to a single audio recording\n",
       "   185       896          0.0      0.0      0.0                      if self._per_file:\n",
       "   186                                                                   feat_extra_frames = self._feature_batch_seq_len - temp_feat.shape[0]\n",
       "   187                                                                   extra_feat = np.ones((feat_extra_frames, temp_feat.shape[1])) * 1e-6\n",
       "   188                                           \n",
       "   189                                                                   label_extra_frames = self._label_batch_seq_len - temp_label.shape[0]\n",
       "   190                                                                   if self._multi_accdoa is True:\n",
       "   191                                                                       extra_labels = np.zeros((label_extra_frames, self._num_track_dummy, self._num_axis, self._num_class))\n",
       "   192                                                                   else:\n",
       "   193                                                                       extra_labels = np.zeros((label_extra_frames, temp_label.shape[1]))\n",
       "   194                                           \n",
       "   195                                                                   for f_row in extra_feat:\n",
       "   196                                                                       self._circ_buf_feat.append(f_row)\n",
       "   197                                                                   for l_row in extra_labels:\n",
       "   198                                                                       self._circ_buf_label.append(l_row)\n",
       "   199                                           \n",
       "   200       896          0.0      0.0      0.0                      file_cnt = file_cnt + 1\n",
       "   201                                           \n",
       "   202                                                           # Read one batch size from the circular buffer\n",
       "   203        42          0.0      0.0      0.0                  feat = np.zeros((self._feature_batch_seq_len, self._nb_mel_bins * self._nb_ch))\n",
       "   204   2688000          0.6      0.0      1.1                  for j in range(self._feature_batch_seq_len):\n",
       "   205   2688000          9.6      0.0     16.8                      feat[j, :] = self._circ_buf_feat.popleft()\n",
       "   206        42          0.0      0.0      0.0                  feat = np.reshape(feat, (self._feature_batch_seq_len, self._nb_ch, self._nb_mel_bins))\n",
       "   207                                           \n",
       "   208        42          0.0      0.0      0.0                  if self._multi_accdoa is True:\n",
       "   209                                                               label = np.zeros((self._label_batch_seq_len, self._num_track_dummy, self._num_axis, self._num_class))\n",
       "   210                                                               for j in range(self._label_batch_seq_len):\n",
       "   211                                                                   label[j, :, :, :] = self._circ_buf_label.popleft()\n",
       "   212                                                           else:\n",
       "   213        42          0.0      0.0      0.1                      label = np.zeros((self._label_batch_seq_len, self._label_len))\n",
       "   214    537600          0.1      0.0      0.2                      for j in range(self._label_batch_seq_len):\n",
       "   215    537600          0.4      0.0      0.7                          label[j, :] = self._circ_buf_label.popleft()\n",
       "   216                                                           # Split to sequences\n",
       "   217        42          0.0      0.0      0.0                  feat = self._split_in_seqs(feat, self._feature_seq_len)\n",
       "   218        42          0.0      0.0      0.0                  feat = np.transpose(feat, (0, 2, 1, 3))\n",
       "   219                                                           \n",
       "   220        42          0.0      0.0      0.0                  label = self._split_in_seqs(label, self._label_seq_len)\n",
       "   221        42          0.0      0.0      0.0                  if self._multi_accdoa is True:\n",
       "   222                                                               pass\n",
       "   223                                                           else:\n",
       "   224        42          0.0      0.0      0.0                      mask = label[:, :, :self._nb_classes]\n",
       "   225        42          0.0      0.0      0.1                      mask = np.tile(mask, 3)\n",
       "   226        42          0.1      0.0      0.1                      label = mask * label[:, :, self._nb_classes:]\n",
       "   227                                           \n",
       "   228        42          0.0      0.0      0.0                  yield feat, label"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -u 1 -f data_gen_train.generate gen_test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S3D Env",
   "language": "python",
   "name": "s3d_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
