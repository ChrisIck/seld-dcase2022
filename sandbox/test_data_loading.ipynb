{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A wrapper script that trains the SELDnet. The training stops when the early stopping metric - SELD error stops improving.\n",
    "#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import cls_feature_class\n",
    "import cls_data_generator\n",
    "import seldnet_model\n",
    "import parameters\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "plot.switch_backend('agg')\n",
    "from IPython import embed\n",
    "from cls_compute_seld_results import ComputeSELDResults, reshape_3Dto2D\n",
    "from SELD_evaluation_metrics import distance_between_cartesian_coordinates\n",
    "import seldnet_model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_accdoa_labels(accdoa_in, nb_classes):\n",
    "    x, y, z = accdoa_in[:, :, :nb_classes], accdoa_in[:, :, nb_classes:2*nb_classes], accdoa_in[:, :, 2*nb_classes:]\n",
    "    sed = np.sqrt(x**2 + y**2 + z**2) > 0.5\n",
    "      \n",
    "    return sed, accdoa_in\n",
    "\n",
    "\n",
    "def get_multi_accdoa_labels(accdoa_in, nb_classes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        accdoa_in:  [batch_size, frames, num_track*num_axis*num_class=3*3*12]\n",
    "        nb_classes: scalar\n",
    "    Return:\n",
    "        sedX:       [batch_size, frames, num_class=12]\n",
    "        doaX:       [batch_size, frames, num_axis*num_class=3*12]\n",
    "    \"\"\"\n",
    "    x0, y0, z0 = accdoa_in[:, :, :1*nb_classes], accdoa_in[:, :, 1*nb_classes:2*nb_classes], accdoa_in[:, :, 2*nb_classes:3*nb_classes]\n",
    "    sed0 = np.sqrt(x0**2 + y0**2 + z0**2) > 0.5\n",
    "    doa0 = accdoa_in[:, :, :3*nb_classes]\n",
    "\n",
    "    x1, y1, z1 = accdoa_in[:, :, 3*nb_classes:4*nb_classes], accdoa_in[:, :, 4*nb_classes:5*nb_classes], accdoa_in[:, :, 5*nb_classes:6*nb_classes]\n",
    "    sed1 = np.sqrt(x1**2 + y1**2 + z1**2) > 0.5\n",
    "    doa1 = accdoa_in[:, :, 3*nb_classes: 6*nb_classes]\n",
    "\n",
    "    x2, y2, z2 = accdoa_in[:, :, 6*nb_classes:7*nb_classes], accdoa_in[:, :, 7*nb_classes:8*nb_classes], accdoa_in[:, :, 8*nb_classes:]\n",
    "    sed2 = np.sqrt(x2**2 + y2**2 + z2**2) > 0.5\n",
    "    doa2 = accdoa_in[:, :, 6*nb_classes:]\n",
    "\n",
    "    return sed0, doa0, sed1, doa1, sed2, doa2\n",
    "\n",
    "\n",
    "def determine_similar_location(sed_pred0, sed_pred1, doa_pred0, doa_pred1, class_cnt, thresh_unify, nb_classes):\n",
    "    if (sed_pred0 == 1) and (sed_pred1 == 1):\n",
    "        if distance_between_cartesian_coordinates(doa_pred0[class_cnt], doa_pred0[class_cnt+1*nb_classes], doa_pred0[class_cnt+2*nb_classes],\n",
    "                                                  doa_pred1[class_cnt], doa_pred1[class_cnt+1*nb_classes], doa_pred1[class_cnt+2*nb_classes]) < thresh_unify:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def test_epoch(data_generator, model, criterion, dcase_output_folder, params, device):\n",
    "    # Number of frames for a 60 second audio with 100ms hop length = 600 frames\n",
    "    # Number of frames in one batch (batch_size* sequence_length) consists of all the 600 frames above with zero padding in the remaining frames\n",
    "    test_filelist = data_generator.get_filelist()\n",
    "\n",
    "    nb_test_batches, test_loss = 0, 0.\n",
    "    model.eval()\n",
    "    file_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_generator.generate():\n",
    "            # load one batch of data\n",
    "            data, target = torch.tensor(data).to(device).float(), torch.tensor(target).to(device).float()\n",
    "\n",
    "            # process the batch of data based on chosen mode\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            if params['multi_accdoa'] is True:\n",
    "                sed_pred0, doa_pred0, sed_pred1, doa_pred1, sed_pred2, doa_pred2 = get_multi_accdoa_labels(output.detach().cpu().numpy(), params['unique_classes'])\n",
    "                sed_pred0 = reshape_3Dto2D(sed_pred0)\n",
    "                doa_pred0 = reshape_3Dto2D(doa_pred0)\n",
    "                sed_pred1 = reshape_3Dto2D(sed_pred1)\n",
    "                doa_pred1 = reshape_3Dto2D(doa_pred1)\n",
    "                sed_pred2 = reshape_3Dto2D(sed_pred2)\n",
    "                doa_pred2 = reshape_3Dto2D(doa_pred2)\n",
    "            else:\n",
    "                sed_pred, doa_pred = get_accdoa_labels(output.detach().cpu().numpy(), params['unique_classes'])\n",
    "                sed_pred = reshape_3Dto2D(sed_pred)\n",
    "                doa_pred = reshape_3Dto2D(doa_pred)\n",
    "\n",
    "            # dump SELD results to the correspondin file\n",
    "            output_file = os.path.join(dcase_output_folder, test_filelist[file_cnt].replace('.npy', '.csv'))\n",
    "            file_cnt += 1\n",
    "            output_dict = {}\n",
    "            if params['multi_accdoa'] is True:\n",
    "                for frame_cnt in range(sed_pred0.shape[0]):\n",
    "                    for class_cnt in range(sed_pred0.shape[1]):\n",
    "                        # determine whether track0 is similar to track1\n",
    "                        flag_0sim1 = determine_similar_location(sed_pred0[frame_cnt][class_cnt], sed_pred1[frame_cnt][class_cnt], doa_pred0[frame_cnt], doa_pred1[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        flag_1sim2 = determine_similar_location(sed_pred1[frame_cnt][class_cnt], sed_pred2[frame_cnt][class_cnt], doa_pred1[frame_cnt], doa_pred2[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        flag_2sim0 = determine_similar_location(sed_pred2[frame_cnt][class_cnt], sed_pred0[frame_cnt][class_cnt], doa_pred2[frame_cnt], doa_pred0[frame_cnt], class_cnt, params['thresh_unify'], params['unique_classes'])\n",
    "                        # unify or not unify according to flag\n",
    "                        if flag_0sim1 + flag_1sim2 + flag_2sim0 == 0:\n",
    "                            if sed_pred0[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred0[frame_cnt][class_cnt], doa_pred0[frame_cnt][class_cnt+params['unique_classes']], doa_pred0[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                            if sed_pred1[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred1[frame_cnt][class_cnt], doa_pred1[frame_cnt][class_cnt+params['unique_classes']], doa_pred1[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                            if sed_pred2[frame_cnt][class_cnt]>0.5:\n",
    "                                if frame_cnt not in output_dict:\n",
    "                                    output_dict[frame_cnt] = []\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred2[frame_cnt][class_cnt], doa_pred2[frame_cnt][class_cnt+params['unique_classes']], doa_pred2[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                        elif flag_0sim1 + flag_1sim2 + flag_2sim0 == 1:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            if flag_0sim1:\n",
    "                                if sed_pred2[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred2[frame_cnt][class_cnt], doa_pred2[frame_cnt][class_cnt+params['unique_classes']], doa_pred2[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred0[frame_cnt] + doa_pred1[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                            elif flag_1sim2:\n",
    "                                if sed_pred0[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred0[frame_cnt][class_cnt], doa_pred0[frame_cnt][class_cnt+params['unique_classes']], doa_pred0[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred1[frame_cnt] + doa_pred2[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                            elif flag_2sim0:\n",
    "                                if sed_pred1[frame_cnt][class_cnt]>0.5:\n",
    "                                    output_dict[frame_cnt].append([class_cnt, doa_pred1[frame_cnt][class_cnt], doa_pred1[frame_cnt][class_cnt+params['unique_classes']], doa_pred1[frame_cnt][class_cnt+2*params['unique_classes']]])\n",
    "                                doa_pred_fc = (doa_pred2[frame_cnt] + doa_pred0[frame_cnt]) / 2\n",
    "                                output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "                        elif flag_0sim1 + flag_1sim2 + flag_2sim0 >= 2:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            doa_pred_fc = (doa_pred0[frame_cnt] + doa_pred1[frame_cnt] + doa_pred2[frame_cnt]) / 3\n",
    "                            output_dict[frame_cnt].append([class_cnt, doa_pred_fc[class_cnt], doa_pred_fc[class_cnt+params['unique_classes']], doa_pred_fc[class_cnt+2*params['unique_classes']]])\n",
    "            else:\n",
    "                for frame_cnt in range(sed_pred.shape[0]):\n",
    "                    for class_cnt in range(sed_pred.shape[1]):\n",
    "                        if sed_pred[frame_cnt][class_cnt]>0.5:\n",
    "                            if frame_cnt not in output_dict:\n",
    "                                output_dict[frame_cnt] = []\n",
    "                            output_dict[frame_cnt].append([class_cnt, doa_pred[frame_cnt][class_cnt], doa_pred[frame_cnt][class_cnt+params['unique_classes']], doa_pred[frame_cnt][class_cnt+2*params['unique_classes']]]) \n",
    "            data_generator.write_output_format_file(output_file, output_dict)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            nb_test_batches += 1\n",
    "            if params['quick_test'] and nb_test_batches == 4:\n",
    "                break\n",
    "\n",
    "\n",
    "        test_loss /= nb_test_batches\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_epoch(data_generator, optimizer, model, criterion, params, device):\n",
    "    nb_train_batches, train_loss = 0, 0.\n",
    "    model.train()\n",
    "    for data, target in data_generator.generate():\n",
    "        # load one batch of data\n",
    "        data, target = torch.tensor(data).to(device).float(), torch.tensor(target).to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # process the batch of data based on chosen mode\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        nb_train_batches += 1\n",
    "        if params['quick_test'] and nb_train_batches == 4:\n",
    "            break\n",
    "\n",
    "    train_loss /= nb_train_batches\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '1', 'test_data_load']\n",
      "Using CPU\n",
      "SET: 1\n",
      "USING DEFAULT PARAMETERS\n",
      "\n",
      "Loading data from /scratch/ci411/SELD_Datasets/TNR_0518\n",
      "\tquick_test: False\n",
      "\tfinetune_mode: False\n",
      "\tpretrained_model_weights: None\n",
      "\tdataset_dir: /scratch/ci411/SELD_Datasets/TNR_0518\n",
      "\tfeat_label_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518\n",
      "\tmodel_dir: models/\n",
      "\tdcase_output_dir: results/\n",
      "\tmode: dev\n",
      "\tdataset: mic\n",
      "\tfs: 24000\n",
      "\thop_len_s: 0.02\n",
      "\tlabel_hop_len_s: 0.1\n",
      "\tmax_audio_len_s: 60\n",
      "\tnb_mel_bins: 64\n",
      "\tuse_salsalite: False\n",
      "\tfmin_doa_salsalite: 50\n",
      "\tfmax_doa_salsalite: 2000\n",
      "\tfmax_spectra_salsalite: 9000\n",
      "\tmulti_accdoa: False\n",
      "\tthresh_unify: 15\n",
      "\tlabel_sequence_length: 50\n",
      "\tbatch_size: 256\n",
      "\tdropout_rate: 0.05\n",
      "\tnb_cnn2d_filt: 64\n",
      "\tf_pool_size: [4, 4, 2]\n",
      "\tnb_rnn_layers: 2\n",
      "\trnn_size: 128\n",
      "\tself_attn: False\n",
      "\tnb_heads: 4\n",
      "\tnb_fnn_layers: 1\n",
      "\tfnn_size: 128\n",
      "\tnb_epochs: 100\n",
      "\tlr: 0.001\n",
      "\taverage: macro\n",
      "\tlad_doa_thresh: 20\n",
      "\tfeature_sequence_length: 250\n",
      "\tt_pool_size: [5, 1, 1]\n",
      "\tpatience: 100\n",
      "\tunique_classes: 14\n",
      "Using Train 1, Val 2, Test 3,4\n"
     ]
    }
   ],
   "source": [
    "argv = ['','1','test_data_load']\n",
    "print(argv)\n",
    "if len(argv) != 3:\n",
    "    print('\\n\\n')\n",
    "    print('-------------------------------------------------------------------------------------------------------')\n",
    "    print('The code expected two optional inputs')\n",
    "    print('\\t>> python seld.py <task-id> <job-id>')\n",
    "    print('\\t\\t<task-id> is used to choose the user-defined parameter set from parameter.py')\n",
    "    print('Using default inputs for now')\n",
    "    print('\\t\\t<job-id> is a unique identifier which is used for output filenames (models, training plots). '\n",
    "          'You can use any number or string for this.')\n",
    "    print('-------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n\\n')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# use parameter set defined by user\n",
    "task_id = '1' if len(argv) < 2 else argv[1]\n",
    "params = parameters.get_params(task_id)\n",
    "params['quick_test'] = True\n",
    "\n",
    "job_id = 1 if len(argv) < 3 else argv[-1]\n",
    "\n",
    "# Training setup\n",
    "train_splits, val_splits, test_splits = None, None, None\n",
    "if params['mode'] == 'dev':\n",
    "    if '2020' in params['dataset_dir']:\n",
    "        test_splits = [1]\n",
    "        val_splits = [2]\n",
    "        train_splits = [[3, 4, 5, 6]]\n",
    "\n",
    "    elif '2021' in params['dataset_dir']:\n",
    "        test_splits = [6]\n",
    "        val_splits = [5]\n",
    "        train_splits = [[1, 2, 3, 4]]\n",
    "\n",
    "    elif '2022' in params['dataset_dir']:\n",
    "        test_splits = [[4]]\n",
    "        val_splits = [[4]]\n",
    "        train_splits = [[1, 2, 3]] \n",
    "\n",
    "    else:\n",
    "        print('Using Train 1, Val 2, Test 3,4')\n",
    "        test_splits = [[3,4]]\n",
    "        val_splits = [[2]]\n",
    "        train_splits = [[1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_name: 1_test_data_load_dev_split0_accdoa_mic_gcc\n",
      "\n",
      "Loading training dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tDatagen_mode: dev, nb_files: 900, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [1]\n",
      "\tbatch_size: 256, feat_seq_len: 250, label_seq_len: 50, shuffle: True\n",
      "\tTotal batches in dataset: 42\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "Loading validation dataset:\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tWARNING: Resetting batch size to 12. To accommodate the inference of longest file of 3000 frames in a single batch\n",
      "\tDatagen_mode: dev, nb_files: 300, nb_classes:14\n",
      "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [2]\n",
      "\tbatch_size: 12, feat_seq_len: 250, label_seq_len: 50, shuffle: False\n",
      "\tTotal batches in dataset: 300\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n",
      "Dumping recording-wise val results in: results/1_test_data_load_dev_split0_accdoa_mic_gcc_20230524164845_val\n"
     ]
    }
   ],
   "source": [
    "#for split_cnt, split in enumerate(test_splits):\n",
    "split_cnt = 0\n",
    "split = test_splits[0]\n",
    "# Unique name for the run\n",
    "loc_feat = params['dataset']\n",
    "\n",
    "if params['dataset'] == 'mic':\n",
    "    if params['use_salsalite']:\n",
    "        loc_feat = '{}_salsa'.format(params['dataset'])\n",
    "    else:\n",
    "        loc_feat = '{}_gcc'.format(params['dataset'])\n",
    "loc_output = 'multiaccdoa' if params['multi_accdoa'] else 'accdoa'\n",
    "\n",
    "cls_feature_class.create_folder(params['model_dir'])\n",
    "unique_name = '{}_{}_{}_split{}_{}_{}'.format(\n",
    "    task_id, job_id, params['mode'], split_cnt, loc_output, loc_feat\n",
    ")\n",
    "model_name = '{}_model.h5'.format(os.path.join(params['model_dir'], unique_name))\n",
    "print(\"unique_name: {}\\n\".format(unique_name))\n",
    "\n",
    "# Load train and validation data\n",
    "print('Loading training dataset:')\n",
    "data_gen_train = cls_data_generator.DataGenerator(\n",
    "    params=params, split=train_splits[split_cnt]\n",
    ")\n",
    "\n",
    "print('Loading validation dataset:')\n",
    "data_gen_val = cls_data_generator.DataGenerator(\n",
    "    params=params, split=val_splits[split_cnt], shuffle=False, per_file=True\n",
    ")\n",
    "\n",
    "# Collect i/o data size and load model configuration\n",
    "data_in, data_out = data_gen_train.get_data_sizes()\n",
    "model = seldnet_model.CRNN(data_in, data_out, params).to(device)\n",
    "\n",
    "if params['finetune_mode']:\n",
    "    print('Running in finetuning mode. Initializing the model to the weights - {}'.format(params['pretrained_model_weights']))\n",
    "    model.load_state_dict(torch.load(params['pretrained_model_weights'], map_location='cpu'))\n",
    "\n",
    "\n",
    "# Dump results in DCASE output format for calculating final scores\n",
    "dcase_output_val_folder = os.path.join(params['dcase_output_dir'], '{}_{}_val'.format(unique_name, strftime(\"%Y%m%d%H%M%S\", gmtime())))\n",
    "cls_feature_class.delete_and_create_folder(dcase_output_val_folder)\n",
    "print('Dumping recording-wise val results in: {}'.format(dcase_output_val_folder))\n",
    "\n",
    "# Initialize evaluation metric class\n",
    "score_obj = ComputeSELDResults(params)\n",
    "\n",
    "# start training\n",
    "best_val_epoch = -1\n",
    "best_ER, best_F, best_LE, best_LR, best_seld_scr = 1., 0., 180., 0., 9999 \n",
    "patience_cnt = 0\n",
    "\n",
    "nb_epoch = 2 if params['quick_test'] else params['nb_epochs']\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "if params['multi_accdoa'] is True:\n",
    "    criterion = seldnet_model.MSELoss_ADPIT()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping recording-wise test results in: results/1_test_data_load_dev_split0_accdoa_mic_gcc_20230524165022_test\n",
      "Computing some stats about the dataset in /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\tWARNING: Resetting batch size to 72. To accommodate the inference of longest file of 17957 frames in a single batch\n",
      "\tDatagen_mode: dev, nb_files: 121, nb_classes:14\n",
      "\tnb_frames_file: 17957, feat_len: 64, nb_ch: 10, label_len:56\n",
      "\n",
      "\tDataset: mic, split: [3, 4]\n",
      "\tbatch_size: 72, feat_seq_len: 250, label_seq_len: 50, shuffle: False\n",
      "\tTotal batches in dataset: 121\n",
      "\tlabel_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_label\n",
      " \tfeat_dir: /scratch/ci411/DCASE_GEN/seld_features/TNR_0518/mic_dev_norm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump results in DCASE output format for calculating final scores\n",
    "dcase_output_test_folder = os.path.join(params['dcase_output_dir'], '{}_{}_test'.format(unique_name, strftime(\"%Y%m%d%H%M%S\", gmtime())))\n",
    "cls_feature_class.delete_and_create_folder(dcase_output_test_folder)\n",
    "print('Dumping recording-wise test results in: {}'.format(dcase_output_test_folder))\n",
    "\n",
    "data_gen_test = cls_data_generator.DataGenerator(\n",
    "            params=params, split=test_splits[split_cnt], shuffle=False, per_file=True\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test_epoch(data_gen_test, model, criterion, dcase_output_test_folder, params, device)\n",
    "\n",
    "use_jackknife=False\n",
    "test_ER, test_F, test_LE, test_LR, test_seld_scr, classwise_test_scr = score_obj.get_SELD_Results(dcase_output_test_folder, is_jackknife=use_jackknife )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S3D Env",
   "language": "python",
   "name": "s3d_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
